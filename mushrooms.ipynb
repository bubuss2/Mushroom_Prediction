{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/mushrooms.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the distribution of edible and poisonous mushrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"Edible\", \"Poisonous\"\n",
    "sizes = [\n",
    "    data.describe()[\"class\"][\"freq\"],\n",
    "    data.describe()[\"class\"][\"count\"] - data.describe()[\"class\"][\"freq\"],\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(\n",
    "    sizes,\n",
    "    labels=labels,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    shadow=True,\n",
    "    startangle=90,\n",
    "    colors=[\"green\", \"magenta\"],\n",
    ")\n",
    "\n",
    "ax.set_title(\"Mushroom Edibility\")\n",
    "ax.legend(labels, loc=\"upper right\")\n",
    "ax.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mushroom edibility by cap shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_shapes = data.groupby([\"cap-shape\", \"class\"]).size().unstack()\n",
    "cap_shapes.fillna(0)\n",
    "cap_shapes_mapping = {\n",
    "    \"b\": \"bell\",\n",
    "    \"c\": \"conical\",\n",
    "    \"x\": \"convex\",\n",
    "    \"f\": \"flat\",\n",
    "    \"k\": \"knobbed\",\n",
    "    \"s\": \"sunken\",\n",
    "}\n",
    "\n",
    "cap_shapes.rename(index=cap_shapes_mapping, inplace=True)\n",
    "\n",
    "ax = cap_shapes.plot.bar(color=[\"green\", \"magenta\"], rot=0, figsize=(12, 7), width=0.8)\n",
    "\n",
    "ax.set_title(\"Mushroom eedibility by cap shape\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"Cap Shape\")\n",
    "labels = [\"Edible\", \"Poisonous\"]\n",
    "\n",
    "ax.legend(labels, loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mushroom edibility by cap surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_surfaces = data.groupby([\"cap-surface\", \"class\"]).size().unstack()\n",
    "cap_surfaces.fillna(0)\n",
    "\n",
    "cap_surfaces_mapping = {\n",
    "    \"f\": \"fibrous\",\n",
    "    \"g\": \"grooves\",\n",
    "    \"s\": \"smooth\",\n",
    "    \"y\": \"scaly\",\n",
    "}\n",
    "\n",
    "cap_surfaces.rename(index=cap_surfaces_mapping, inplace=True)\n",
    "\n",
    "ax = cap_surfaces.plot.bar(color=[\"green\", \"magenta\"], rot=0, figsize=(12, 7), width=0.8)\n",
    "\n",
    "ax.set_title(\"Mushroom edibility by cap surface\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"Cap Surface\")\n",
    "labels = [\"Edible\", \"Poisonous\"]\n",
    "\n",
    "ax.legend(labels, loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mushroom edibility by color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_colors = data.groupby([\"cap-color\", \"class\"]).size().unstack()\n",
    "cap_colors.fillna(0)\n",
    "\n",
    "cap_colors_mapping = {\n",
    "    \"n\": \"brown\",\n",
    "    \"b\": \"buff\",\n",
    "    \"c\": \"cinnamon\",\n",
    "    \"g\": \"gray\",\n",
    "    \"r\": \"green\",\n",
    "    \"p\": \"pink\",\n",
    "    \"u\": \"purple\",\n",
    "    \"e\": \"red\",\n",
    "    \"w\": \"white\",\n",
    "    \"y\": \"yellow\",\n",
    "}\n",
    "\n",
    "cap_colors.rename(index=cap_colors_mapping, inplace=True)\n",
    "\n",
    "ax = cap_colors.plot.bar(color=[\"green\", \"magenta\"], rot=0, figsize=(12, 7), width=0.8)\n",
    "\n",
    "ax.set_title(\"Mushroom edibility by cap color\")\n",
    "ax.set_xlabel(\"Cap Color\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "labels = [\"Edible\", \"Poisonous\"]\n",
    "ax.legend(labels, loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mushroom edibility by bruises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruises = data.groupby([\"bruises\", \"class\"]).size().unstack()\n",
    "\n",
    "bruise_mapping = {\"f\": \"no bruises\", \"t\": \"bruises\"}\n",
    "\n",
    "bruises.rename(index=bruise_mapping, inplace=True)\n",
    "\n",
    "ax = bruises.plot.bar(color=[\"green\", \"magenta\"], rot=0, figsize=(12, 7), width=0.8)\n",
    "\n",
    "ax.set_title(\"Mushroom edibility by present bruises\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "labels = [\"Edible\", \"Poisonous\"]\n",
    "plt.legend(labels, loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare utility functions to imitiate missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_data(data, missing_ratio):\n",
    "    \"\"\"\n",
    "    Create random missing data in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        data (pandas.DataFrame): The input DataFrame.\n",
    "        missing_ratio (float): The ratio of missing values to create (between 0 and 1).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with random missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    data_with_missing = data.copy()\n",
    "    num_missing_values = int(data.size * missing_ratio)\n",
    "    missing_indices = np.random.choice(data.size, size=num_missing_values, replace=False)\n",
    "    data_with_missing.values.flat[missing_indices] = np.nan\n",
    "\n",
    "    return data_with_missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_most_common(col):\n",
    "    col.fillna(col.value_counts().index[0], inplace=True)\n",
    "    return col\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(\"class\", axis=1),\n",
    "    data[\"class\"],\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=data[\"class\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = create_missing_data(X_train, 0.7)\n",
    "X_train = X_train.apply(lambda col: fill_with_most_common(col))\n",
    "\n",
    "X_test = create_missing_data(X_test, 0.7)\n",
    "X_test = X_test.apply(lambda col: fill_with_most_common(col))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace edible with 1 and poisonous with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map({\"e\": 0, \"p\": 1})\n",
    "y_test = y_test.map({\"e\": 0, \"p\": 1})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            data.drop(\"class\", axis=1).select_dtypes(exclude=\"object\").columns,\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            data.drop(\"class\", axis=1).select_dtypes(include=\"object\").columns,\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", xgb_model)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the parameters with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [50, 100, 150],\n",
    "    \"classifier__learning_rate\": [0.1, 0.01, 0.001],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(xgb_pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CategoryEncoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test data into validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation, X_test, y_validation, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, shuffle=True, random_state=42, stratify=y_test\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode input data with OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "X_train_onehot = onehot_encoder.fit_transform(X_train)\n",
    "X_test_onehot = onehot_encoder.transform(X_test)\n",
    "X_validation_onehot = onehot_encoder.transform(X_validation)\n",
    "\n",
    "print(X_train_onehot.shape[1])\n",
    "print(X_test_onehot.shape[1])\n",
    "print(X_validation_onehot.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the layers for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = Sequential()\n",
    "\n",
    "neural_network.add(Dense(units=16, activation=\"elu\", input_shape=(X_train_onehot.shape[1],)))\n",
    "neural_network.add(Dense(units=16, activation=\"elu\"))\n",
    "neural_network.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "neural_network.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_history = neural_network.fit(\n",
    "    X_train_onehot,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_validation_onehot, y_validation),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.evaluate(X_test_onehot, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nn_history.history).plot(figsize=(12, 7))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    CategoryEncoding,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    Input,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the data with OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "X_train_onehot = onehot_encoder.fit_transform(X_train)\n",
    "X_test_onehot = onehot_encoder.transform(X_test)\n",
    "X_validation_onehot = onehot_encoder.transform(X_validation)\n",
    "\n",
    "print(X_train_onehot.shape[1])\n",
    "print(X_test_onehot.shape[1])\n",
    "print(X_validation_onehot.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the layers for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D Layer (filters=16, kernel_size=3, strides=1, padding=\"same\"):\n",
    "\n",
    "    This is a 1D convolutional layer that performs convolution on the input data.\n",
    "    It uses 16 filters, each of size 3, to extract 16 different features from the input.\n",
    "    The stride of 1 means the filters move one step at a time.\n",
    "    Padding=\"same\" ensures that the output has the same spatial dimensions as the input.\n",
    "\n",
    "BatchNormalization:\n",
    "\n",
    "    This layer normalizes the activations of the previous layer.\n",
    "    It helps in stabilizing and accelerating the training process by reducing the internal covariate shift.\n",
    "\n",
    "Activation(\"relu\"):\n",
    "\n",
    "    ReLU (Rectified Linear Unit) is an activation function that introduces non-linearity.\n",
    "    It applies the element-wise rectified linear activation to the output of the previous layer.\n",
    "\n",
    "MaxPooling1D Layer (pool_size=2, strides=2):\n",
    "\n",
    "    This layer performs max pooling, which downsamples the input by taking the maximum value in each region.\n",
    "    It uses a pool size of 2, meaning it takes the maximum value within a window of size 2.\n",
    "    The stride of 2 means the pooling window moves two steps at a time.\n",
    "\n",
    "Conv1D Layer (filters=32, kernel_size=3, strides=1, padding=\"same\"):\n",
    "\n",
    "    This is another 1D convolutional layer with 32 filters and a kernel size of 3.\n",
    "    It extracts 32 different features from the input using convolution.\n",
    "\n",
    "BatchNormalization:\n",
    "\n",
    "    Again, this layer normalizes the activations of the previous layer.\n",
    "\n",
    "Activation(\"relu\"):\n",
    "\n",
    "    Applies ReLU activation to the output of the previous layer.\n",
    "\n",
    "MaxPooling1D Layer (pool_size=2, strides=2):\n",
    "\n",
    "    Performs max pooling with a pool size of 2 and stride of 2.\n",
    "\n",
    "Flatten:\n",
    "\n",
    "    This layer flattens the previous layer's output into a 1D vector.\n",
    "    It converts the multi-dimensional feature maps into a single continuous vector.\n",
    "\n",
    "Dense Layer (units=16, activation=\"elu\"):\n",
    "\n",
    "    Adds a fully connected layer with 16 units (neurons).\n",
    "    It connects every neuron in this layer to every neuron in the previous layer.\n",
    "    The activation function used here is ELU (Exponential Linear Unit).\n",
    "\n",
    "Dense Layer (units=16, activation=\"elu\"):\n",
    "\n",
    "    Adds another fully connected layer with 16 units and ELU activation.\n",
    "\n",
    "Dense Layer (units=1, activation=\"sigmoid\"):\n",
    "\n",
    "    Adds a final fully connected layer with 1 unit and sigmoid activation.\n",
    "    Sigmoid activation is commonly used for binary classification problems to produce a probability output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()\n",
    "\n",
    "# create convolutional neural network\n",
    "\n",
    "cnn.add(\n",
    "    Conv1D(\n",
    "        filters=16,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        input_shape=(X_train_onehot.shape[1], 1),\n",
    "    )\n",
    ")\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Activation(\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(\n",
    "    Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Activation(\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(units=16, activation=\"elu\"))\n",
    "cnn.add(Dense(units=16, activation=\"elu\"))\n",
    "cnn.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "cnn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided code, the input shape of the CNN is defined as (X_train_onehot.shape[1], 1), which means the network expects input data with shape (num_features, 1). However, the one-hot encoded data (X_train_onehot and X_validation_onehot) have a shape of (num_samples, num_features).\n",
    "\n",
    "To match the expected input shape, the reshape operation is applied to the input data.\n",
    "\n",
    "This reshapes the input data to have dimensions (num_samples, num_features, 1). The additional dimension of size 1 represents the channel dimension, which is required for convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_history = cnn.fit(\n",
    "    X_train_onehot.reshape(X_train_onehot.shape[0], X_train_onehot.shape[1], 1),\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(\n",
    "        X_validation_onehot.reshape(X_validation_onehot.shape[0], X_validation_onehot.shape[1], 1),\n",
    "        y_validation,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.evaluate(X_test_onehot.reshape(X_test_onehot.shape[0], X_test_onehot.shape[1], 1), y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cnn_history.history).plot(figsize=(12, 7))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
